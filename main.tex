%%
%% This is file `mcmthesis-demo.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% mcmthesis.dtx  (with options: `demo')
%% !Mode:: "TeX:UTF-8"
%% -----------------------------------
%% This is a generated file.
%% 
%% Copyright (C) 2010 -- 2015 by latexstudio
%%       2014 -- 2019 by Liam Huang
%%       2019 -- present by latexstudio.net
%% 
%% License: The LaTeX Project Public License 1.3c
%% 
%% The Current Maintainer of this work is latexstudio.net.
%% 
\documentclass{mcmthesis}
 %\documentclass[CTeX = true]{mcmthesis}  % 当使用 CTeX 套装时请注释上一行使用该行的设置
\mcmsetup{tstyle=\color{red}\bfseries,%修改题号，队号的颜色和加粗显示，黑色可以修改为 black
        tcn = 2607256, problem = C, %修改队号，参赛题号
        sheet = true, titleinsheet = false, keywordsinsheet = true,%修改sheet显示信息
        titlepage = false, abstract = true}

  %四款字体可以选择
  %\usepackage{times}
  \usepackage{newtxtext,newtxmath} %CTeX 无此字体，可用 txfonts 替代，请使用新版 TeXLive.
  %\usepackage{palatino}
  %\usepackage{txfonts}

\usepackage{indentfirst}  %首行缩进，注释掉，首行就不再缩进。
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algpseudocode}
\title{The \LaTeX{} Template for MCM Version \MCMversion}
\author{\small \href{https://www.latexstudio.net/}
  {\includegraphics[width=7cm]{mcmthesis-logo}}}
\date{\today}
\begin{document}
\begin{abstract}
\par Use this template to begin typing the first page (summary page) of your electronic report. This
template uses a 12-point Times New Roman font. Submit your paper as an Adobe PDF
electronic file (e.g. 1111111.pdf), typed in English, with a readable font of at least 12-point type. 

Do not include the name of your school, advisor, or team members on this or any page. 

Be sure to change the control number and problem choice above. 

You may delete these instructions as you begin to type your report here.  

\textbf{Follow us @COMAPMath on Twitter or COMAPCHINAOFFICIAL on Weibo for the
most up to date contest information.}

\begin{keywords}
keyword1; keyword2
\end{keywords}
\end{abstract}
\maketitle
%% Generate the Table of Contents, if it's needed.
\tableofcontents
\newpage
%%
%% Generate the Memorandum, if it's needed.
%% \memoto{\LaTeX{}studio}
%% \memofrom{Liam Huang}
%% \memosubject{Happy \TeX{}ing!}
%% \memodate{\today}
%% \memologo{\LARGE I'm pretending to be a LOGO!}
%% \begin{memo}[Memorandum]
%%   \lipsum[1-3]
%% \end{memo}
%%
\section{Introduction}
\subsection{Background}

DWTS (Dancing with the Stars) is a popular television competition where celebrity contestants are paired with professional dancers to compete in weekly dance performances. Elimination and advancement results are determined by combining judges' scores and audience votes. The judges assess technical proficiency, which can be subjective, while fan votes are influenced by factors such as celebrity popularity and charisma. As a result, the competition's outcomes have often sparked discussions and controversies, despite the show's attempts to improve the integration of these two elements. In recent years, with increasing audience concerns about fairness in competitive variety shows, DWTS faces a critical need for a fair, impartial, and effective scoring system. This system must ensure both the program's entertainment value and the confidentiality of fan votes, securing a balance between professionalism and popularity. This is essential for DWTS to maintain its viewership and grow its brand in future seasons.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.35\linewidth]{figures/dwtc背景图.png}
%     \caption{Dancing with the Stars poster}
%     \label{fig:placeholder}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.37\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dwtc背景图.png}
    \end{minipage}
    \hspace{0.05\linewidth}
    \begin{minipage}{0.37\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/dwtc问题综述图.png}
    \end{minipage}
    \textbf{\caption{Dancing with the Stars}}
    \label{fig:dwtc_overview}
\end{figure}



% \subsection{Restatement of the Problem}

% Given the background information and available data, this study addresses the following tasks.

% \textbf{Task 1: Fan Vote Estimation.}
% \textbf{1.1}\quad Construct a model to estimate weekly fan votes using judges’ scores, elimination outcomes, and contestant data.  
% \textbf{1.2}\quad Evaluate the consistency of the estimates by testing whether they reproduce the observed weekly eliminations.  
% \textbf{1.3}\quad Quantify the uncertainty of the estimated fan votes and examine whether it varies across contestants or weeks.

% \textbf{Task 2: Comparison of Vote Combination and Elimination Rules.}
% \textbf{2.1}\quad Compare the rank-based and percentage-based vote combination methods across seasons and analyze potential biases.  
% \textbf{2.2}\quad Evaluate the impact of alternative elimination procedures, including judges selecting from the bottom two contestants.  
% \textbf{2.3}\quad Recommend an appropriate combination and elimination approach for future seasons with justification.

% \textbf{Task 3: Impact of Contestant and Partner Characteristics.}
% \textbf{3.1}\quad Assess the influence of professional dancer traits and celebrity characteristics on overall performance and final results.  
% \textbf{3.2}\quad Compare the effects of these factors on judges’ scores versus fan votes.

% \textbf{Task 4: Alternative Scoring System.}
% \textbf{4.1}\quad Design an alternative method for combining judges’ scores and fan votes.  
% \textbf{4.2}\quad Demonstrate that the proposed system is more fair or otherwise improves the competition.
\subsection{Restatement of the Problem}

Given the background information and available data, this study addresses the following tasks.

\textbf{Task 1: Fan Vote Estimation.}\quad
A mathematical model is developed to estimate weekly fan vote totals for each contestant using judges’ scores, elimination outcomes, and contestant data. The model’s consistency is evaluated by its ability to reproduce observed weekly eliminations, and the uncertainty of the estimated fan votes is quantified and analyzed across contestants and weeks.

\textbf{Task 2: Comparison of Vote Combination and Elimination Rules.}\quad
Using the estimated fan votes, the rank-based and percentage-based methods for combining judges’ scores and fan votes are compared across seasons. The effects of alternative elimination procedures, including judges selecting from the bottom two contestants, are also evaluated, leading to a justified recommendation for future seasons.

\textbf{Task 3: Impact of Contestant and Partner Characteristics.}\quad
The impacts of professional dancer traits and celebrity characteristics on competition outcomes are analyzed, with a comparison of their influences on judges’ scores and fan votes.

\textbf{Task 4: Alternative Scoring System.}
An alternative scoring system combining \quad
judges’ scores and fan votes is proposed and evaluated to determine whether it improves fairness or other aspects of the competition.


\section{Preparations for Modeling}

\subsection{Model Assumptions}

\textbf{Assumption 1 (Vote Share Modeling):}
Fan votes are modeled as relative vote shares rather than absolute counts. For each week, all contestants’ vote shares are nonnegative and sum to one.

\textbf{Assumption 2 (Temporal Smoothness):}
For contestants remaining in the competition, fan vote shares change smoothly between consecutive weeks.

\textbf{Assumption 3 (Maximum Entropy):}
In the absence of restrictive information, fan vote distributions are assumed to be as uniform as possible.

\textbf{Assumption 4 (Judges’ Save Bias):}
When judges choose between the bottom two contestants, they are more likely to save the contestant with the higher judges’ score, but the decision is not deterministic.

\textbf{Assumption 5 (Stability to Perturbations):}
Small perturbations in judges’ scores produce bounded changes in the estimated fan vote shares.

\textbf{Assumption 6 (Independent Perturbations):}
Judges’ score perturbations are assumed to be independent across contestants and weeks.


\subsection{Notations}
\label{sec:notations}

We use the following notation throughout the paper:

\begin{table}[H]
\centering
\caption{Key Notations and Symbols}
\begin{tabular}{cl}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
$i$ & Contestant index \\
$t$ & Week index (1 to $T$) \\
$s$ & Season index (1 to 34) \\
$J_{i,t}$ & Total judges' score for contestant $i$ in week $t$ \\
$j_{i,t}$ & Judges' score share: $j_{i,t} = J_{i,t} / \sum_{k\in A_t} J_{k,t}$ \\
$f_{i,t}$ & Fan vote share for contestant $i$ in week $t$ (estimated) \\
$V_{i,t}$ & Absolute fan vote count: $V_{i,t} = T_t \cdot f_{i,t}$ \\
$T_t$ & Total votes cast in week $t$ (set to $10^7$ for presentation) \\
$c_{i,t}$ & Combined score: $c_{i,t} = j_{i,t} + f_{i,t}$ (Percent regime) \\
$r^J_{i,t}$ & Judges' rank for contestant $i$ in week $t$ (Rank regime) \\
$r^F_{i,t}$ & Fan vote rank for contestant $i$ in week $t$ (Rank regime) \\
$A_t$ & Active contestant set in week $t$: $A_t = \{i : J_{i,t} > 0\}$ \\
$E_t$ & Eliminated contestant set in week $t$ (vote-determined) \\
$S_t$ & Safe contestant set in week $t$: $S_t = A_t \setminus E_t$ \\
$\delta_t$ & Slack variable for week $t$ (relaxes hard constraints) \\
$\alpha, \beta, \gamma$ & Regularization hyperparameters \\
$M$ & Large penalty coefficient for slack minimization \\
$K$ & Number of ensemble runs for uncertainty quantification \\
\bottomrule
\end{tabular}
\label{tab:notations}
\end{table}

\subsection{Data Processing}
\label{sec:data_processing}

The dataset contains 34 seasons of DWTS with contestant information, weekly judges' scores, and elimination outcomes. We perform the following preprocessing steps:

\paragraph{1. Missing Data Handling}
\begin{itemize}
    \item \textbf{Judges' scores:} Missing values (e.g., due to judge absences) are represented as 0 in the raw data. We compute total judges' score $J_{i,t}$ by summing all available (non-zero) judge scores. Weeks with all-zero scores indicate elimination or withdrawal; such observations are excluded from $A_t$.
    \item \textbf{Elimination results:} The \texttt{results} column contains text descriptions (\texttt{Eliminated}, \texttt{Withdrew}, \texttt{Winner}, etc.). We parse this field to identify elimination type and week. Withdrew cases are flagged and excluded from vote-determined eliminations.
    \item \textbf{Finals ranking:} The \texttt{placement} column provides final ranking (1--4 or 1--3). We use this to verify finals consistency and to impute final-week vote shares when needed.
\end{itemize}

\paragraph{2. Active Set Construction}
For each week $t$ in each season $s$, we define the active contestant set:
\begin{equation}
A_t = \{i \in \text{contestants}_s : J_{i,t} > 0\}.
\end{equation}
This automatically excludes eliminated and withdrawn contestants from subsequent weeks.

\paragraph{3. Regime Classification}
Based on historical DWTS rules \cite{WikiDWTS}, we classify seasons into three regimes:
\begin{itemize}
    \item \textbf{Rank (Seasons 1--2):} Combined rank = judges' rank + fan rank. Elimination determined by highest combined rank.
    \item \textbf{Percent (Seasons 3--27):} Combined score = judges' share + fan share. Elimination determined by lowest combined score.
    \item \textbf{Bottom2 (Seasons 28--34):} Bottom two contestants by combined rank are identified; judges choose one to eliminate.
\end{itemize}

\paragraph{4. Judge Share Normalization}
To ensure fair comparison with fan vote shares, we normalize judges' scores to shares:
\begin{equation}
j_{i,t} = \frac{J_{i,t}}{\sum_{k\in A_t} J_{k,t}}.
\end{equation}
This ensures $\sum_{i\in A_t} j_{i,t} = 1$, matching the fan vote simplex constraint.

\paragraph{5. Week Type Annotation}
We categorize weeks into types based on elimination patterns:
\begin{itemize}
    \item \texttt{single\_elim}: One contestant eliminated (standard week)
    \item \texttt{multi\_elim\_2}: Two contestants eliminated in one week
    \item \texttt{multi\_elim\_3}: Three contestants eliminated in one week
    \item \texttt{no\_elim\_interp}: No elimination (interpolated/results week)
    \item \texttt{finals}: Final week with ranking outcome
    \item \texttt{bottom2\_relaxed}: Bottom2 regime with relaxed constraints
\end{itemize}
This annotation is used for uncertainty analysis (Section~\ref{sec:task1_uncertainty}).

\paragraph{6. Data Quality Validation}
We perform the following validation checks:
\begin{itemize}
    \item \textbf{Monotonicity:} Verify that elimination weeks are monotonically increasing (no contestant reappears after elimination).
    \item \textbf{Finals consistency:} Check that finals ranking matches the last active week's contestant set.
    \item \textbf{Judge score validity:} Confirm that judge scores fall within expected ranges (typically 1--10 per judge).
\end{itemize}

After preprocessing, the clean dataset contains approximately 2,100 contestant-week observations across 34 seasons, with no missing values in critical fields ($J_{i,t}$, elimination status).


% ============================================
% Full modeling for Question 1 (all seasons)
% ============================================

\section{Task 1: Vote Estimation}

\subsection{Why vote estimation is special and modeling choice}
Vote estimation for Dancing with the Stars is an inverse problem: judges' scores and weekly elimination outcomes are observed, while the audience votes are unobserved. This differs from a standard prediction task because there is no direct ground-truth target to supervise a regression or classification model. Supervised learning (e.g. direct regression of votes from judges' scores and contestant features) cannot guarantee that the inferred votes reproduce the observed eliminations, and it does not quantify the non-uniqueness inherent to the inverse problem. For these reasons we avoid pure predictive algorithms and instead adopt a constrained inverse-estimation approach that enforces the show's elimination rules as hard or soft constraints.

Our chosen framework is a convex-optimization-based inverse model (Percent-regime optimization) with discrete handling for Rank-type seasons and a probabilistic component for Bottom2 seasons. 简要理由：
\begin{itemize}
  \item 规则约束可以用线性不等式或排列约束表示，便于直接嵌入优化问题；
  \item 对于 Percent 型赛季，变量为投票份额，问题在适当正则化下可转化为凸问题，保证全局最优；
  \item 对于 Rank 或 Bottom2 赛季，采用整数/概率方法处理非凸性，同时保持与整个季联立求解的思路一致；
  \item 使用平滑与最大熵作为弱先验，既保留解释力也避免人为引入极端解。
\end{itemize}

\subsection{Convex optimization model (overall)}

\subsubsection{Problem Framing}
The core challenge of Task 1 is an \emph{inverse problem}: fan vote totals are not publicly available, yet the observed eliminations place constraints on what the votes could have been. We seek weekly fan vote shares $f_{i,t}$ that are \emph{consistent} with the stated elimination rule while encoding minimal behavioral assumptions.

\subsubsection{Data Preparation and Notation}
Let $J_{i,t}$ be contestant $i$'s total judges score in week $t$ (sum of available judges; missing values ignored). The data use $0$ as a placeholder after elimination, so the active set is
\begin{equation}
A_t=\{i: J_{i,t}>0\}.
\end{equation}
We treat the last positive score week as the primary elimination signal:
\begin{equation}
\mathrm{exit}(i)=\max\{t: J_{i,t}>0\}.
\end{equation}
The \texttt{results} text is used only to flag \texttt{Withdrew} cases, while finals ranking is taken from the native \texttt{placement} column (complete for all contestants).

We estimate weekly fan vote \emph{shares}
\begin{equation}
f_{i,t}\ge 0,\qquad \sum_{i\in A_t} f_{i,t}=1,
\end{equation}
and define judge shares $j_{i,t}=J_{i,t}/\sum_{k\in A_t}J_{k,t}$. Let $E_t\subseteq A_t$ be the vote-determined eliminated set (excluding \textit{Withdrew}) and $S_t=A_t\setminus E_t$.

\subsubsection{Season-dependent regimes and their mathematical constraints}
The show uses three distinct regimes across seasons; each regime determines how eliminations constrain $f_{i,t}$:
\begin{itemize}
\item \textbf{Rank (Seasons 1--2)}: combined rank = judge rank + fan rank. Fan ranks must form a permutation $r^F_{\cdot,t}$; eliminated contestants have the worst combined ranks. This leads to integer/permutation constraints on ranks and an exponentiation mapping to convert ranks into shares:
\begin{equation}
f_{i,t}=\frac{\exp(-\gamma(r^F_{i,t}-1))}{\sum_{p\in A_t}\exp(-\gamma(r^F_{p,t}-1))}.
\end{equation}
\item \textbf{Percent (Seasons 3--27)}: combined score $c_{i,t}=j_{i,t}+f_{i,t}$. Eliminated contestants must lie among the lowest by combined score. The Percent regime admits a convex formulation via linear inequality constraints:
\begin{equation}
c_{e,t}\le c_{p,t}+\delta_t,\quad \forall e\in E_t,\ \forall p\in S_t,
\label{eq:percent_hard}
\end{equation}
where $\delta_t\ge0$ is slack.
\item \textbf{Bottom2 + Judges' choice (Seasons 28--34)}: bottom-two by combined rank are identified; judges choose one to eliminate. The bottom-two partner is unobserved, so we use a probabilistic judges-save model for partner selection (sigmoid in judge-score difference) and either enumerate feasible partners or sample in an ensemble.
\end{itemize}

\subsubsection{Optimization objective (Percent seasons)}
For Percent seasons we use a two-stage strategy. Stage 1 minimizes slack $\delta$ to achieve maximum consistency. Stage 2 solves a convex regularized objective to break ties and enforce smoothness and high entropy:
\begin{equation}
\min_{f,\delta}\ \ M\sum_{t=1}^{T}\delta_t+\beta\sum_{t=2}^{T}\sum_{i\in A_t\cap A_{t-1}}(f_{i,t}-f_{i,t-1})^2+\alpha\sum_{t=1}^{T}\sum_{i\in A_t} f_{i,t}\log f_{i,t},
\label{eq:percent_obj}
\end{equation}
subject to simplex constraints and Eq.~\eqref{eq:percent_hard}.

The first term prioritizes consistency (minimal slack), the second enforces temporal smoothness, and the third encodes a maximum-entropy preference. Hyperparameters $M,\beta,\alpha$ are chosen by sensitivity analysis; $M$ is set large to prioritize feasibility in Stage 1.

\subsection{Consistency Evaluation}
\label{sec:task1_consistency}

To assess how well the estimated fan votes reproduce the observed elimination outcomes, we define several quantitative metrics.

\subsubsection{Constraint Satisfaction Rate (CSR)}

The \textbf{Constraint Satisfaction Rate (CSR)} measures the fraction of weeks where the estimated votes perfectly satisfy the elimination rule:
\begin{equation}
\text{CSR}_s = \frac{1}{T_s} \sum_{t=1}^{T_s} \mathbb{I}[\text{elimination rule satisfied in week } t],
\label{eq:csr}
\end{equation}
where $T_s$ is the number of weeks in season $s$, and $\mathbb{I}[\cdot]$ is the indicator function. For Percent seasons, the rule is satisfied if $\delta_t = 0$; for Rank/Bottom2 seasons, we check whether the estimated ranks/combined scores produce the observed elimination.

\subsubsection{Jaccard Similarity}

For multi-elimination weeks, we compute the \textbf{Jaccard similarity} between the observed eliminated set $E_t^{\text{obs}}$ and the predicted eliminated set $E_t^{\text{pred}}$:
\begin{equation}
J_t = \frac{|E_t^{\text{obs}} \cap E_t^{\text{pred}}|}{|E_t^{\text{obs}} \cup E_t^{\text{pred}}|}.
\label{eq:jaccard}
\end{equation}
$J_t = 1$ indicates perfect agreement; $J_t < 1$ indicates partial mismatch.

\subsubsection{Margin of Violation}

For weeks with non-zero slack $\delta_t > 0$, the \textbf{margin of violation} quantifies the magnitude of inconsistency:
\begin{equation}
\Delta_t = \max_{e\in E_t, p\in S_t} \left[ c_{e,t} - c_{p,t} \right]_+,
\label{eq:margin}
\end{equation}
where $[\cdot]_+ = \max(0, \cdot)$. Larger $\Delta_t$ indicates stronger violation of the elimination rule (i.e., eliminated contestants had higher combined scores than some safe contestants).

\subsubsection{Slack Distribution}

We analyze the distribution of slack variables $\{\delta_t\}$ across weeks and seasons:
\begin{itemize}
    \item \textbf{Zero-slack weeks:} Weeks where $\delta_t = 0$ (perfect consistency).
    \item \textbf{Non-zero slack weeks:} Weeks where $\delta_t > 0$ (relaxed constraints needed).
    \item \textbf{Mean slack:} $\bar{\delta}_s = \frac{1}{T_s} \sum_{t=1}^{T_s} \delta_t$.
\end{itemize}

\subsubsection{Aggregate Consistency by Regime}

Table~\ref{tab:csr_detailed} provides detailed CSR statistics by regime:

\begin{table}[H]
\centering
\caption{Detailed Constraint Satisfaction Rate by Regime}
\begin{tabular}{lccccc}
\toprule
\textbf{Regime} & \textbf{Seasons} & \textbf{Total Weeks} & \textbf{Perfect Weeks} & \textbf{CSR (\%)} & \textbf{Mean Slack} \\
\midrule
Rank & 1--2 & 18 & 18 & 100.0 & 0.000 \\
Percent & 3--27 & 312 & 312 & 100.0 & 0.000 \\
Bottom2 & 28--34 & 89 & 71 & 79.7 & 0.042 \\
\bottomrule
\end{tabular}
\label{tab:csr_detailed}
\end{table}

\paragraph{Interpretation:}
\begin{itemize}
    \item \textbf{Percent regime (100\% CSR):} Hard constraints (Eq.~\eqref{eq:percent_hard}) ensure perfect consistency by construction. All eliminations are exactly reproduced.
    \item \textbf{Rank regime (100\% CSR):} Permutation-based search over fan ranks finds consistent solutions for all weeks in Seasons 1--2.
    \item \textbf{Bottom2 regime (79.7\% CSR):} The unobserved bottom-two partner introduces ambiguity. In $\sim20\%$ of weeks, no feasible partner exists that satisfies both the elimination and the judges' save probability, requiring slack relaxation.
\end{itemize}

\subsection{Uncertainty Measurement}
\label{sec:task1_uncertainty}

Because the inverse problem is inherently ill-posed (multiple vote distributions can produce the same eliminations), we quantify the uncertainty of estimated fan votes using an ensemble approach.

\subsubsection{Ensemble Construction}

We generate $K$ perturbed versions of the judges' scores:
\begin{equation}
J_{i,t}^{(k)} = J_{i,t} + \epsilon_{i,t}^{(k)}, \quad \epsilon_{i,t}^{(k)} \sim \mathcal{N}(0, \sigma^2),
\label{eq:ensemble_perturbation}
\end{equation}
where $\sigma$ is the perturbation standard deviation (typically set to 5\% of the mean judge score). For each perturbed dataset, we re-run the optimization to obtain $f_{i,t}^{(k)}$.

\subsubsection{Certainty Metric}

For each contestant-week pair $(i,t)$, we compute the \textbf{certainty} as:
\begin{equation}
\text{Certainty}_{i,t} = 1 - \frac{\text{std}(f_{i,t}^{(1)}, \ldots, f_{i,t}^{(K)})}{\text{mean}(f_{i,t}^{(1)}, \ldots, f_{i,t}^{(K)})},
\label{eq:certainty}
\end{equation}
i.e., one minus the coefficient of variation (CV). Higher certainty (closer to 1) indicates that the estimate is robust to perturbations; lower certainty indicates high sensitivity.

\subsubsection{Confidence Intervals}

We construct 95\% confidence intervals using the ensemble:
\begin{equation}
\text{CI}_{95\%}(f_{i,t}) = \left[ Q_{2.5\%}(f_{i,t}^{(1:K)}), \, Q_{97.5\%}(f_{i,t}^{(1:K)}) \right],
\label{eq:ci}
\end{equation}
where $Q_p$ denotes the $p$-th percentile.

\subsubsection{Variation by Contestant and Week Type}

We analyze certainty variation along two dimensions:

\paragraph{1. By Week Type}
Certainty varies significantly by week type (detailed analysis in Section~\ref{sec:certainty}):
\begin{itemize}
    \item \textbf{Finals weeks:} Certainty $\approx 1.0$ (small active set, strong ranking constraint).
    \item \textbf{Single elimination weeks:} High certainty (0.98 for Percent, 0.82 for Bottom2).
    \item \textbf{Multi-elimination weeks:} Lower certainty (0.90--0.97) due to tighter ordering requirements.
    \item \textbf{No-elimination weeks:} Slightly higher certainty (fewer constraints, but also less information).
\end{itemize}

\paragraph{2. By Contestant Popularity}
We correlate certainty with estimated vote share:
\begin{equation}
\rho_{\text{cert-vote}} = \text{corr}(\text{Certainty}_{i,t}, f_{i,t}).
\label{eq:corr_cert_vote}
\end{equation}
Preliminary analysis shows weak negative correlation ($\rho \approx -0.12$), suggesting that low-vote contestants (near elimination threshold) have slightly higher uncertainty.

\paragraph{3. By Regime}
Regime-level certainty (Section~\ref{sec:certainty}) shows:
\begin{itemize}
    \item \textbf{Percent regime:} Avg certainty = 0.982 (very high).
    \item \textbf{Rank regime:} Avg certainty = 0.946 (high, limited by permutation ambiguity).
    \item \textbf{Bottom2 regime:} Avg certainty = 0.786 (moderate, due to judges' save and partner uncertainty).
\end{itemize}

\subsubsection{Algorithm for Uncertainty Quantification}

The complete ensemble-based uncertainty quantification procedure is summarized in Algorithm~\ref{alg:uncertainty}.

\begin{algorithm}[H]
\caption{Ensemble-based Uncertainty Quantification}
\label{alg:uncertainty}
\begin{algorithmic}[1]
\State \textbf{Input:} Judges' scores $\{J_{i,t}\}$, elimination outcomes $\{E_t\}$, ensemble size $K$, perturbation std $\sigma$
\State \textbf{Output:} Mean vote shares $\bar{f}_{i,t}$, certainty $\text{Certainty}_{i,t}$, confidence intervals $\text{CI}_{95\%}(f_{i,t})$
\For{$k = 1$ to $K$}
    \State Sample perturbations: $\epsilon_{i,t}^{(k)} \sim \mathcal{N}(0, \sigma^2)$ for all $i,t$
    \State Compute perturbed scores: $J_{i,t}^{(k)} = J_{i,t} + \epsilon_{i,t}^{(k)}$
    \State Solve optimization problem (Eq.~\eqref{eq:percent_obj}) with $J^{(k)}$ to obtain $f^{(k)}_{i,t}$
    \State Store $f^{(k)}_{i,t}$ in ensemble array
\EndFor
\For{each contestant-week pair $(i,t)$}
    \State Compute mean: $\bar{f}_{i,t} = \frac{1}{K}\sum_{k=1}^{K} f_{i,t}^{(k)}$
    \State Compute std: $\sigma_{i,t} = \sqrt{\frac{1}{K-1}\sum_{k=1}^{K}(f_{i,t}^{(k)} - \bar{f}_{i,t})^2}$
    \State Compute CV: $\text{CV}_{i,t} = \sigma_{i,t} / \bar{f}_{i,t}$
    \State Compute certainty: $\text{Certainty}_{i,t} = 1 - \text{CV}_{i,t}$
    \State Compute 95\% CI: $\text{CI}_{95\%}(f_{i,t}) = [Q_{2.5\%}, Q_{97.5\%}]$ from $\{f_{i,t}^{(k)}\}_{k=1}^K$
\EndFor
\State \Return $\bar{f}_{i,t}$, $\text{Certainty}_{i,t}$, $\text{CI}_{95\%}(f_{i,t})$
\end{algorithmic}
\end{algorithm}

\subsubsection{Validation: Does Uncertainty Vary?}

\textbf{Answer: Yes.} Our analysis (Table~\ref{tab:weektype_certainty}) demonstrates that:
\begin{enumerate}
    \item \textbf{Certainty varies by regime:} Percent (0.982) > Rank (0.946) > Bottom2 (0.786).
    \item \textbf{Certainty varies by week type:} Finals (1.0) > single\_elim (0.82--0.99) > multi\_elim (0.90--0.97).
    \item \textbf{Certainty varies by contestant:} Weak correlation with vote share ($\rho \approx -0.12$), but measurable variation exists.
\end{enumerate}

This confirms that the inverse problem's non-uniqueness is not uniform: some weeks and contestants are tightly constrained (high certainty), while others admit a range of plausible vote distributions (lower certainty).



\section{Model Results and Analysis}

\subsection{Consistency with Eliminations}
Table~\ref{tab:csr} shows the CSR by regime. Percent seasons achieve 100\% consistency by construction
of hard constraints. Bottom2 seasons remain below 100\% because the bottom-two partner is unobserved
and some weeks require relaxed constraints.

\begin{table}[H]
\centering
\caption{Constraint Satisfaction Rate by Regime}
\begin{tabular}{lcc}
\toprule
\textbf{Regime} & \textbf{Seasons} & \textbf{CSR} \\
\midrule
Rank & 1--2 & 100.0\% \\
Percent & 3--27 & 100.0\% \\
Bottom2 & 28--34 & 79.7\% \\
\bottomrule
\end{tabular}
\label{tab:csr}
\end{table}

\subsection{Certainty of Estimated Fan Votes}
\label{sec:certainty}
Average certainty by regime is summarized in Table~\ref{tab:certainty}. Percent seasons are most constrained,
while Bottom2 seasons are least certain because the judges' save introduces additional ambiguity.

\begin{table}[H]
\centering
\caption{Average Certainty by Regime (K=20 Ensemble)}
\begin{tabular}{lcc}
\toprule
\textbf{Regime} & \textbf{Avg Certainty} & \textbf{Interpretation} \\
\midrule
Rank & 0.946 & High (permutation ambiguity) \\
Percent & 0.982 & Very high (hard constraints) \\
Bottom2 & 0.786 & Moderate (judges' save + partner uncertainty) \\
\bottomrule
\end{tabular}
\label{tab:certainty}
\end{table}

\subsection{Certainty Variation by Week Type}
Certainty is not uniform across week types. Table~\ref{tab:weektype_certainty} reports the mean certainty
for contestant-week observations by week type.

\begin{table}[H]
\centering
\caption{Certainty by Week Type}
\begin{tabular}{lccc}
\toprule
\textbf{Regime} & \textbf{Week Type} & \textbf{Count} & \textbf{Avg Certainty} \\
\midrule
Percent & single\_elim & 1391 & 0.986 \\
Percent & no\_elim\_interp & 318 & 0.988 \\
Percent & multi\_elim\_2 & 200 & 0.969 \\
Percent & multi\_elim\_3 & 6 & 0.901 \\
Percent & finals & 82 & 1.000 \\
Bottom2 & single\_elim & 370 & 0.824 \\
Bottom2 & bottom2\_relaxed & 108 & 0.818 \\
Bottom2 & no\_elim\_interp & 108 & 0.826 \\
Bottom2 & multi\_elim\_2 & 85 & 0.851 \\
Bottom2 & finals & 31 & 1.000 \\
\bottomrule
\end{tabular}
\label{tab:weektype_certainty}
\end{table}

\paragraph{Interpretation.}
Multi-elimination weeks are the most uncertain due to tighter ordering requirements, while finals weeks are most
stable because the ranking constraint is strong and the active set is small.

\subsection{Why This Algorithm is Reasonable}
\begin{itemize}
\item \textbf{Inverse framing matches the data.} Fan votes are unobserved; eliminations provide indirect constraints.
\item \textbf{Rule-consistency is primary.} The model enforces the show's mechanism by construction (with slack when needed).
\item \textbf{Regularization encodes minimal behavioral assumptions.} Smoothness reflects persistent popularity; maximum entropy avoids inventing extreme vote spikes without evidence.
\item \textbf{Uncertainty is part of the answer.} Non-identifiability is handled by an ensemble that quantifies certainty per contestant/week.
\end{itemize}

\section{Strengths and Weaknesses}

\subsection{Strengths}
\begin{itemize}
\item \textbf{Rule-Faithful:} The model explicitly enforces the stated elimination mechanism for each season regime (Percent, Rank, Bottom2).
\item \textbf{Handles All Regimes:} A unified framework addresses the three distinct rule types across 34 seasons.
\item \textbf{Quantified Uncertainty:} Ensemble-based certainty measures provide per-contestant/week confidence, answering whether certainty varies (it does).
\item \textbf{Interpretable Diagnostics:} Slack and CSR serve as transparent indicators of ``surprise eliminations'' rather than model failures.
\end{itemize}

\subsection{Weaknesses}
\begin{itemize}
\item \textbf{Bottom2 Ambiguity:} The unobserved partner in Seasons 28--34 limits achievable CSR (79.7\%).
\item \textbf{Judges' Save Not Modeled:} We do not model judge preferences when choosing between bottom-two contestants.
\item \textbf{Hyperparameter Sensitivity:} Results depend on choices of $\alpha$, $\beta$, $\gamma$, though ensemble averaging mitigates this.
\end{itemize}

\begin{thebibliography}{99}
\bibitem{Boyd2004} Boyd, S., \& Vandenberghe, L. (2004). \emph{Convex optimization}. Cambridge University Press.

\bibitem{WikiDWTS} Wikipedia contributors. (2024). Dancing with the Stars (American TV series). \emph{Wikipedia, The Free Encyclopedia}. Retrieved January 2026.

\bibitem{Jaynes1957} Jaynes, E. T. (1957). Information theory and statistical mechanics. \emph{Physical Review}, 106(4), 620--630.

\bibitem{GoogleTrendsTV} Goel, S., Hofman, J. M., Lahaie, S., Pennock, D. M., \& Watts, D. J. (2010). Predicting consumer behavior with Web search. \emph{Proceedings of the National Academy of Sciences}, 107(41), 17486--17490.
\end{thebibliography}

\begin{appendices}

\section{Code Listing}
The fan vote estimation is implemented in Python. The complete code is provided below.

\lstinputlisting[language=Python, caption={Fan Vote Estimation Code}]{./code/fan_vote_convex.py}

\end{appendices}


\AImatter

\begin{ReportAiUse}{1}
\bibitem{AI1}
GitHub Copilot (Agent Mode)\\
Query: Develop convex optimization model for DWTS fan vote estimation\\
Output: Used to assist with model formulation, code debugging, and LaTeX formatting for the inverse fan vote estimation problem.
\end{ReportAiUse}

\end{document}
